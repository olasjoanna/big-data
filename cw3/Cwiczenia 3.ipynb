{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f1eb294-585d-4cd5-b6f5-7527ae399fdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Names.csv \n",
    "* Dodaj kolumnę z wartością czasu wykonania notatnika w formacie epoch\n",
    "* Dodaj kolumnę w której wyliczysz wzrost w stopach (feet)\n",
    "* Odpowiedz na pytanie jakie jest najpopularniesze imię?\n",
    "* Dodaj kolumnę i policz wiek aktorów \n",
    "* Usuń kolumny (bio, death_details)\n",
    "* Zmień nazwy kolumn - dodaj kapitalizaję i usuń _\n",
    "* Posortuj dataframe po imieniu rosnąco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "014992b7-71ad-47c0-97cc-13d2d2382769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./3. Pobierz Dane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "682ee105-0a36-455f-9c09-98dfba93ba21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filePath = \"dbfs:/FileStore/tables/Files/names.csv\"\n",
    "namesDf = spark.read.format(\"csv\") \\\n",
    "              .option(\"header\",\"true\") \\\n",
    "              .option(\"inferSchema\",\"true\") \\\n",
    "              .load(filePath) \n",
    "\n",
    "#display(namesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16a431a5-ede7-4fc0-95d1-5d6cc2c5b75c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sprawdzamy plan wykonania \n",
    "namesDf.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5546466-d945-42bd-84c9-1b461a1eab38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dodaj kolumnę z wartością czasu wykonania notatnika w formacie epoch\n",
    "\n",
    "from pyspark.sql.functions import unix_timestamp, current_timestamp\n",
    "namesDf=namesDf.withColumn(\"execution_time\", unix_timestamp(current_timestamp()))\n",
    "#display(namesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e9413f-f5b8-4c7d-b6c7-f12b31c57759",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dodaj kolumnę w której wyliczysz wzrost w stopach (feet)\n",
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "namesDf=namesDf.withColumn(\"height_in_feet\", round(col(\"height\")/30.48 , 2))\n",
    "#display(namesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7d0cf6-9cbd-47ad-847a-2c20a1189f8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Odpowiedz na pytanie jakie jest najpopularniesze imię?\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "most_freq_name=namesDf.groupBy(\"name\").agg(count(\"*\").alias(\"count\")) \\\n",
    "  .orderBy(col(\"count\").desc()) \\\n",
    "    .first()\n",
    "most_freq_name['name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "292da06a-3ef0-42bc-8cc4-2e10d7f4abe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dodaj kolumnę i policz wiek aktorów\n",
    "#jak data smierci i urodzenia jest dostepna otherwise null \n",
    "from pyspark.sql.functions import when, col, datediff, floor\n",
    "\n",
    "namesDf = namesDf.withColumn(\n",
    "    \"age\",\n",
    "    when(col(\"date_of_birth\").isNotNull() & col(\"date_of_death\").isNotNull(),\n",
    "         floor(datediff(col(\"date_of_death\"), col(\"date_of_birth\")) / 365))\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "#display(namesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d3e5691-e99a-4d49-a8d5-9e0b82e3e6cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Usuń kolumny (bio, death_details)\n",
    "namesDf = namesDf.drop(\"bio\", \"death_details\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f45448c-6d1c-4bb5-9730-67e1094f3c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Zmień nazwy kolumn - dodaj kapitalizaję i usuń _\n",
    "\n",
    "new_columns = [col(c).alias(c.title().replace(\"_\", \"\")) for c in namesDf.columns]\n",
    "namesDf = namesDf.select(*new_columns)\n",
    "\n",
    "#namesDf.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "694e65bc-4fce-4511-b92f-7f8f4076ea67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Posortuj dataframe po imieniu rosnąco\n",
    "namesDf = namesDf.orderBy(\"Name\")\n",
    "#display(namesDf)\n",
    "#namesDf.orderBy(\"Name\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f86d9d14-5103-4d4a-8df2-cec361db347d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Movies.csv\n",
    "* Dodaj kolumnę z wartością czasu wykonania notatnika w formacie epoch\n",
    "* Dodaj kolumnę która wylicza ile lat upłynęło od publikacji filmu\n",
    "* Dodaj kolumnę która pokaże budżet filmu jako wartość numeryczną, (trzeba usunac znaki walut)\n",
    "* Usuń wiersze z dataframe gdzie wartości są null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc238ac0-9f22-4685-b2d2-1a074e813334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filePath = \"dbfs:/FileStore/tables/Files/movies.csv\"\n",
    "moviesDf = spark.read.format(\"csv\") \\\n",
    "              .option(\"header\",\"true\") \\\n",
    "              .option(\"inferSchema\",\"true\") \\\n",
    "              .load(filePath) \\\n",
    "\n",
    "#display(moviesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a893751b-0f41-41a8-a68f-c77f0310b59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "moviesDf.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "489ce46c-6f9c-43bc-b98e-d99668126431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dodaj kolumnę z wartością czasu wykonania notatnika w formacie epoch\n",
    "moviesDf = moviesDf.withColumn(\"execution_time\", unix_timestamp(current_date()))\n",
    "#display(moviesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead67a8a-ee39-4ca0-a067-a5a91ef0cdfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dodaj kolumnę która wylicza ile lat upłynęło od publikacji filmu\n",
    "from pyspark.sql.functions import to_date, year, current_date, col, when\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "moviesDf = moviesDf.withColumn(\n",
    "    \"parsed_date\",\n",
    "    when(col(\"date_published\").rlike(r'^\\d{4}$'), to_date(col(\"date_published\"), \"yyyy\"))  # Format yyyy\n",
    "    .when(col(\"date_published\").rlike(r'^\\d{2}\\.\\d{2}\\.\\d{4}$'), to_date(col(\"date_published\"), \"dd.MM.yyyy\"))  # Format dd.MM.yyyy\n",
    "    .otherwise(to_date(col(\"date_published\"), \"yyyy-MM-dd\"))  # Inne przypadki np. yyyy-MM-dd\n",
    ")\n",
    "\n",
    "# Dodanie kolumny z wyliczeniem lat, które upłynęły od publikacji filmu\n",
    "moviesDf = moviesDf.withColumn(\n",
    "    \"years_since_published\",\n",
    "    year(current_date()) - year(col(\"parsed_date\"))\n",
    ")\n",
    "\n",
    "moviesDf=moviesDf.drop('parsed_date')\n",
    "\n",
    "#display(moviesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66479532-a432-462c-be3c-e308208bc61d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dodaj kolumnę która pokaże budżet filmu jako wartość numeryczną, (trzeba usunac znaki walut)\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "moviesDf = moviesDf.withColumn(\n",
    "    \"budget_numeric\",\n",
    "    regexp_replace(col(\"budget\"), r'[^\\d]', '').cast('int')\n",
    ")\n",
    "#display(moviesDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f17223ad-5423-47d4-963d-479c1c39a398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Usuń wiersze z dataframe gdzie wartości są null\n",
    "moviesDf = moviesDf.dropna()\n",
    "#display(moviesDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d1e437-12d6-41c0-a8f4-d430ae74431f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ratings.csv\n",
    "* Dodaj kolumnę z wartością czasu wykonania notatnika w formacie epoch\n",
    "* Dla każdego z poniższych wyliczeń nie bierz pod uwagę `nulls` \n",
    "* Kto daje lepsze oceny chłopcy czy dziewczyny dla całego setu\n",
    "* Dla jednej z kolumn zmień typ danych do `long` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75c22c14-a54e-4727-b2ad-f8b631f2b541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filePath = \"dbfs:/FileStore/tables/Files/ratings.csv\"\n",
    "ratingsDf = spark.read.format(\"csv\") \\\n",
    "              .option(\"header\",\"true\") \\\n",
    "              .option(\"inferSchema\",\"true\") \\\n",
    "              .load(filePath) \\\n",
    "\n",
    "#display(ratingsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4676a7f-fc2d-4248-a933-51e015f815dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dodaj kolumnę z wartością czasu wykonania notatnika w formacie epoch\n",
    "ratingsDf=ratingsDf.withColumn(\"execution_time\", unix_timestamp(current_timestamp()))\n",
    "#display(ratingsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4100d9b-4976-4329-81dd-64ee9ca1a13d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Kto daje lepsze oceny chłopcy czy dziewczyny dla całego setu\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "ratingsDf_noNull=ratingsDf.dropna(subset=[\"males_allages_avg_vote\", \"females_allages_avg_vote\"])\n",
    "\n",
    "avg_male=ratingsDf_noNull.select(mean(\"males_allages_avg_vote\")).collect()[0][0]\n",
    "avg_female=ratingsDf_noNull.select(mean(\"females_allages_avg_vote\")).collect()[0][0]\n",
    "\n",
    "if avg_male>avg_female:\n",
    "  print(\"Lepsze oceny daja chlopcy\\n\")\n",
    "else:\n",
    "  print(\"Lepsze oceny daja dziewczyny\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c54688-a89d-425e-9313-41429a1b33ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dla jednej z kolumn zmień typ danych do long\n",
    "ratingsDf=ratingsDf.withColumn(\"total_votes_long\", col(\"total_votes\").cast(\"long\"))\n",
    "#display(ratingsDf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75ceac85-8ed1-4a5a-b566-60edaa2cba02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ZADANIE 2\n",
    "Spark UI\n",
    "\n",
    "Jobs – Zakładka ta zawiera listę wszystkich uruchomionych zadań, wraz z ich unikalnymi identyfikatorami, opisami, datą wykonania oraz czasem trwania. W tym miejscu można również znaleźć informacje o liczbie etapów, które zakończyły się sukcesem, oraz o liczbie zadań, które zostały wykonane pomyślnie. Dodatkowo dostępny jest Event Timeline, który pokazuje wykonanie zadań w czasie, umożliwiając śledzenie postępu aplikacji.\n",
    "\n",
    "Stages – W tej sekcji znajdują się szczegółowe informacje dotyczące poszczególnych etapów aplikacji, w tym identyfikatory etapów, ich opisy, czas trwania oraz status sukcesu. Można tu sprawdzić, które etapy zakończyły się pomyślnie, a które napotkały błędy.\n",
    "\n",
    "Storage – Ta zakładka pokazuje dane dotyczące przechowywania danych w pamięci. Oferuje wgląd w dane strukturalne, takie jak DataFrame, RDD czy inne struktury danych przechowywane w pamięci podręcznej. Można tu również zobaczyć ilość danych przechowywanych w pamięci przez aplikację.\n",
    "\n",
    "Environment – W tej sekcji znajdują się informacje o środowisku Spark, w tym szczegóły dotyczące wersji Javy, Scali oraz zmiennych środowiskowych. Dzięki tym informacjom użytkownicy mogą lepiej zrozumieć konfigurację systemu, na którym działa aplikacja Spark.\n",
    "\n",
    "Executors – Ta zakładka prezentuje dane o wykonawcach zadań (executors), umożliwiając porównanie wykorzystania pamięci przez poszczególnych wykonawców, czasu ich pracy oraz ilości przetworzonych danych. Informacje te pozwalają na monitorowanie efektywności wykonawców i pomagają w optymalizacji zadań. Dodatkowo, dostępne są informacje o wykonawcach aktywnych oraz tych, którzy zakończyli swoje zadania.\n",
    "\n",
    "SQL/DataFrame – Sekcja ta wyświetla zapytania SQL, które zostały wykonane w ramach aplikacji, wraz z czasem ich wykonania, identyfikatorem oraz opisem. Dzięki temu użytkownicy mogą śledzić i analizować zapytania SQL oraz DataFrame, co pozwala na lepsze zrozumienie i optymalizację zapytań.\n",
    "\n",
    "JDBC/ODBC Server – Zakładka ta pokazuje statystyki dotyczące sesji JDBC/ODBC, w tym informacje o wykonanych zapytaniach SQL. Może być przydatna do monitorowania zapytań wykonywanych z zewnętrznych źródeł i baz danych.\n",
    "\n",
    "Structured Streaming – W tej sekcji prezentowane są informacje dotyczące zapytań streamingowych. Umożliwia to śledzenie postępu w czasie rzeczywistym, a także monitorowanie statusu zapytań streamingowych w trakcie ich wykonywania.\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5872f62-f69d-4c63-9b70-aff882dbb925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#ZADANIE 3\n",
    "moviesDf.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066f312c-3753-4865-9e54-cb73d20a4d78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "groupedDf = moviesDf.groupBy(\"genre\").agg(F.avg(\"avg_vote\").alias(\"average_vote\"))\n",
    "\n",
    "groupedDf.explain(True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Cwiczenia 3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
