{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7db26596-eaec-46a5-a556-a076e4c3169a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def file_exists(path):\n",
    "  try:\n",
    "    dbutils.fs.ls(path)\n",
    "    return True \n",
    "  except Exception as e:\n",
    "    if 'java.io.FileNotFoundException' in str(e):\n",
    "      return False\n",
    "    else:\n",
    "      raise\n",
    "\n",
    "actorsUrl = \"https://raw.githubusercontent.com/cegladanych/azure_bi_data/main/IMDB_movies/actors.csv\"\n",
    "\n",
    "filePath = \"/FileStore/tables/Files/\"\n",
    "dbutils.fs.mkdirs(filePath)\n",
    "actorsFile = \"actors.csv\"\n",
    "tmp = \"file:/tmp/\"\n",
    "dbfsdestination = \"dbfs:/FileStore/tables/Files/\"\n",
    "\n",
    "#ACTORS\n",
    "import urllib.request\n",
    "\n",
    "if (file_exists(filePath + actorsFile) == False):\n",
    "  urllib.request.urlretrieve(actorsUrl,\"/tmp/\" + actorsFile)\n",
    "  dbutils.fs.mv(tmp + actorsFile,dbfsdestination + actorsFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ad8ffa7-cc93-4f12-abf3-112ef64589bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(dbfsdestination + actorsFile, header=True, inferSchema=True)\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab64c187-90a5-49c1-90ee-62394812312b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " from pyspark.sql.functions import col, explode, regexp_replace, regexp_extract, ifnull, array_contains, count, avg, max, lit, split, nullif\n",
    "\n",
    " #fill\n",
    "nonull1= df.fillna({\"job\": \"unknown\"})\n",
    "\n",
    " # drop\n",
    "nonull3=df.dropna(subset=[\"characters\"])\n",
    "\n",
    " #regexp_replace\n",
    " cleared=df.withColumn(\"characters\", regexp_replace(col(\"characters\"), \"[\\[\\]]\",\"\"))\n",
    "\n",
    " # explode\n",
    "exploded= cleared.withColumn(\"character_list\", split(col(\"characters\"), r\"\\) \\[|\\],\\s*\"))\n",
    "exploded=exploded.withColumn(\"exploded_characters\", explode(col(\"character_list\")))\n",
    "\n",
    "\n",
    " # regexp_extract \n",
    " extracted=cleared.withColumn(\"main_character\", regexp_extract(col(\"characters\"),\"^(.*?)( -|$)\", 1))\n",
    " #ifnull\n",
    "nonull2 = cleared.withColumn(\"job\", ifnull(col(\"job\"), lit(\"unknown\")))\n",
    " #nullIf\n",
    "with_nulls= nonull2.withColumn(\"job\", nullif(col(\"job\"), lit(\"unknown\")))\n",
    " # replace\n",
    " replaced=cleared.withColumn(\"category\", regexp_replace(col(\"category\"), \"actor\", \"performer\"))\n",
    " # array_contains.  \n",
    "writers = df.withColumn(\"is_writer\", array_contains(split(col(\"category\"), \" \"), \"writer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1c6684f-ea13-46c6-9de7-b36196646a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+------------------+------------+\n|           category|total_actors|      avg_ordering|max_ordering|\n+-------------------+------------+------------------+------------+\n|            actress|      133414|2.7871137961533274|          10|\n|           producer|      101092| 7.744035136311479|          10|\n|             writer|      122793| 6.809052633293429|          10|\n|           composer|       66861| 8.345567670241246|          10|\n|           director|       88968| 5.096585289092707|          10|\n|               self|         909|3.0693069306930694|          10|\n|              actor|      222337|2.7522004884477167|          10|\n|             editor|       33780| 9.286234458259326|          10|\n|    cinematographer|       55423|  8.80094906446782|          10|\n|production_designer|        9485|   9.5222983658408|          10|\n|    archive_footage|         444|3.7545045045045047|          10|\n|      archive_sound|           7|               2.0|           3|\n+-------------------+------------+------------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Użyj 3 funkcji agregujących (które według Ciebie są najciekawsze).  \n",
    "\n",
    "aggregated= df.groupBy(\"category\").agg(\n",
    "    count(\"imdb_name_id\").alias(\"total_actors\"),\n",
    "    avg(\"ordering\").alias(\"avg_ordering\"),\n",
    "    max(\"ordering\").alias(\"max_ordering\")\n",
    ")\n",
    "aggregated.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e56a79a-b5a5-4d93-ba8e-3876b59a8c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import udf, col, pandas_udf\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "#Stwórz 2 funkcje UDF do wybranego zestawu danych, wymyśl co mają robić w #kontekście wybranych danych. Jedna funkcja z dekoratorem @pandas_udf druga #standardowa. \n",
    "\n",
    "#Jedna funkcja działające na typach liczbowych: int, double  \n",
    "def count_length(characters):\n",
    "    if characters is None:\n",
    "        return 0\n",
    "    return sum(len(c) for c in characters)\n",
    "\n",
    "\n",
    "count_length_udf = udf(count_length, IntegerType())\n",
    "\n",
    "df = df.withColumn(\"characters_length\", count_length_udf(col(\"characters\")))\n",
    "\n",
    "\n",
    "#Jedna funkcja na string \n",
    "@pandas_udf(StringType())\n",
    "def uppercase_category_udf(category_series: pd.Series) -> pd.Series:\n",
    "    return category_series.str.upper()\n",
    "    \n",
    "df = df.withColumn(\"category_uppercase\", uppercase_category_udf(col(\"category\")))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notatnik 1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}